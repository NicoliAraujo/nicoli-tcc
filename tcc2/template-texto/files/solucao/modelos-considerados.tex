%!TEX root = ../../novoIndex.tex
Levando em conta a adoção de CNNs como o modelo de ML a ser usado neste trabalho, considerou-se inicialmente a utilização das arquiteturas LeNet e AlexNet. A implementação da AlexNet seguiu a prática atual de utilizar apenas uma GPU em seu treinamento, então as camadas dividas no trabalho original foram unificadas \cite{tensorflow:alexnet}. Todas as funções de ativação tangente hiperbólica disponíveis nas versões originais destas redes foram substituídas pela função \emph{ReLU}, por ser mais eficiente computacionalmente, por evitar que o gradiente descendente fique preso em pontos de sela e por promover uma convergência mais rápida \cite{maas2013rectifier}. Adotou-se um \emph{batch size} igual a $64$ para o treinamento, e o método de otimização do gradiente descendente foi o \emph{Adam}. O número de épocas e a taxa de aprendizado foram obtidas de maneira experimental, observando a perda obtida ao final de cada época.

A seguir, duas redes mais profundas foram aplicadas para a tarefa: a VGG-16 e a SqueezeNet. A arquitetura profunda, porém direta, da VGG-16 foi utilizada em uma tarefa de estimação de idade descrita em \cite{rothe2015dex} e obteve resultados satisfatórios, como mencionado na seção \ref{sec:trab_relac}. A SqueezeNet é uma CNN apresentada em 2018, com o intuito de obter o mesmo desempenho da AlexNet, mas com 50 vezes menos parâmetros. No contexto de uma aplicação que necessita prover ao usuário respostas em tempo real através de um \emph{hardware} limitado, as características desta rede podem se mostrar satisfatórias \cite{squeezenet}.

A fim de caracterizar a tarefa de regressão proposta, as camadas de saída das redes consideradas, que consistem de múltiplos neurônios voltados à classificação, foram substituídas por apenas um neurônio. Em um primeiro momento, o neurônio de saída era seguido por uma função de ativação \emph{ReLU}, e as imagens de entrada não estavam normalizadas ou equalizadas. Após análise dos resultados preliminares obtidos para estes modelos iniciais, substituiu-se a \emph{ReLU} da camada de saída por uma de suas variantes, chamada \emph{Leaky ReLU}, e expressa na Figura \ref{fig:lrelu}, e as imagens foram normalizadas e equalizadas. A taxa de aprendizado inicial foi padronizada em um valor de $10^{-3}$ com decaimento de $10^{-10}$ para ambas as redes.

\begin{figure}[!ht]
     \centering
     \includegraphics[width=0.3\textwidth]{img/lrelu}
     \caption{Função de Ativação \emph{Leaky ReLU}}
     \label{fig:lrelu}
\end{figure}

Na seção a seguir estão os resultados preliminares obtidos do treino dos modelos, hiperparâmetros e estratégias supracitados.
