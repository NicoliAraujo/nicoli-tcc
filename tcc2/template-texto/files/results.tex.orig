%!TEX root = ../novoIndex.tex

Considerando a abordagem descrita na solução proposta, os resultados da execução das CNNs aplicadas ao problema de estimação de idade a partir de uma imagem de face são apresentados a seguir.

%\input{files/results-tcc1}

\subsection{Abordagem 1}
A primeira abordagem de treinamento das CNNs utilizou as imagens da base de dados com equalização por histograma de frequência, normalizadas, e com $50\%$ de chance de estarem rotacionadas horizontalmente. Conforme mencionado na Seção \ref{subsec:modelos}, os treinamentos e testes compreenderam as arquiteturas canônicas LeNet e AlexNet com funções de ativação \emph{ReLU} e \emph{LeakyRelU} nas camadas ocultas e de ativação. Obedecendo ao método de validação cruzada \emph{holdout} previamente mencionado, os resultados da etapa de teste foram obtidos, e estão detalhados na Tabela
Não foram utilizadas técnicas de transfer learning.

<<<<<<< HEAD
=======
- Normalização das imagens
- 1 técnica de data augmentation: horizontal flip

(\ldots)
>>>>>>> 7287c22bb31ec1eff67afb9f84f8c7a1378bd888
% Explicação da le net, gráficos da le net

% Explicação da alexnet, gráficos da alexnet


%% Sumário
%% Tabela
%  Rede | Função de ativação | Parâmetros | Tempo de treinamento | RMSE obtido | MAE obtido


\begin{figure}[hb!]
	\caption{Redes neurais biológicas.}
	\begin{subfigure}[hb]{0.5\linewidth}
		\caption{Treinamento Alexnet LRelU com imagens normalizadas e equalizadas}
		\label{fig:histalexlrelunorm}
    \centering
		\includegraphics[width=\linewidth]{img/graficos-fase2/fig-history-alexnet-lrelu-data-augmentation-2-2.png}
	\end{subfigure}
	\begin{subfigure}[hb]{0.5\linewidth}
		\caption{Treinamento Alexnet ReLU com imagens normalizadas e equalizadas}
		\label{fig:redeneuralbiologica}
		\includegraphics[width=\linewidth]{img/graficos-fase2/fig-history-alexnet-relu-data-augmentation-2-1.png}
	\end{subfigure}\\
  \begin{subfigure}[hb]{0.5\linewidth}
    \caption{Treinamento LeNet ReLU com imagens normalizadas e equalizadas}
    \label{fig:redeneuralbiologica}
    \includegraphics[width=\linewidth]{img/graficos-fase2/fig-history-lenet-relu-data-augmentation-2-1.png}%
  \end{subfigure}%
  \begin{subfigure}[hb]{0.5\linewidth}
    \caption{Treinamento LeNet RelU com imagens normalizadas e equalizadas}
    \label{fig:redeneuralbiologica}
    \includegraphics[width=\linewidth]{img/graficos-fase2/fig-history-lenet-relu-data-augmentation-2-1.png}
  \end{subfigure}%
\end{figure}


\begin{figure}[hb!]
	\caption{Redes neurais biológicas.}
	\begin{subfigure}[hb]{0.5\linewidth}
		\caption{Reta-0 Alexnet LRelU com imagens normalizadas e equalizadas}
		\label{fig:histalexlrelunorm}
		\includegraphics[width=\linewidth]{img/graficos-fase2/fig-reta-0-alexnet-lrelu-data-augmentation-2-2.png}
	\end{subfigure}%
	\begin{subfigure}[hb]{0.5\linewidth}
		\caption{Reta-0 Alexnet ReLU com imagens normalizadas e equalizadas}
		\label{fig:redeneuralbiologica}
		\includegraphics[width=\linewidth]{img/graficos-fase2/fig-reta-0-alexnet-relu-data-augmentation-2-1.png}
	\end{subfigure}\\
  \begin{subfigure}[hb]{0.5\linewidth}
    \caption{Reta-0 LeNet ReLU com imagens normalizadas e equalizadas}
    \label{fig:redeneuralbiologica}
    \includegraphics[width=\linewidth]{img/graficos-fase2/fig-reta-0-lenetregressor-relu-data-augmentation-2-1.png}%
  \end{subfigure}%
  \begin{subfigure}[hb]{0.5\linewidth}
    \caption{Reta-0 LeNet RelU com imagens normalizadas e equalizadas}
    \label{fig:redeneuralbiologica}
   \includegraphics[width=\linewidth]{img/graficos-fase2/fig-reta-0-lenetregressor-relu-data-augmentation-2-1.png}
  \end{subfigure}%
\end{figure}



\subsection{Abordagem 2}

- Mesmas redes
- Normalização das imagens, equalização por histograma -> o que é
- data augmentation ->  mais técnicas de data augmentation

\subsection{Abordagem 3}

Outras arquiteturas
VGG
com transfer learning
1. Retirar última camada (softmax) e adicionar leaky relu
2. Retirar duas últimas camadas (dense e softmax) e adicionar leaky relu
