%!TEX root = ../sbc-template.tex
O objetivo deste trabalho consiste em elaborar estratégias inteligentes para estimação de idade de telespectadores de  \emph{Smart} TVs a partir de suas respectivas fotografias faciais. Para este fim, foram propostos, treinados e testados em caráter preliminar dois modelos de CNNs já bem estabelecidos na literatura, a LeNet e AlexNet, com dois perfis de hiperparâmetros cada um.

Ao observar as previsões realizadas pelas primeiras configurações de LeNet e AlexNet propostas, notou-se que ambas exibiam saídas iguais a zero para quaisquer imagens de entrada. Concluiu-se que as redes estavam sofrendo do chamado \emph{ReLU dying problem}, ou problema da morte da \emph{ReLU}. Considerando o gráfico desta função de ativação exibido na Tabela \ref{tab:ativacoes}, nota-se que a \emph{ReLU} exibe saída $0$ para entradas com valores negativos, e saída linear para entradas positivas.
%Sua derivada, também exibida na Tabela \ref{tab:ativações}, é igual a zero para valores negativos e 1 para valores positivos, o que impede os gradientes dos parâmetros da rede calculados na fase \emph{backwards} do treinamento, exibida no Algoritmo \ref{alg:backpropagation}, de tenderem a zero, o que causaria a estagnação da atualização dos parâmetros e consequentemente do aprendizado do modelo.
Apesar dos benefícios da utilização desta função de ativação, os valores de entrada negativos geram saídas e gradientes nulos, o que significa que os parâmetros correspondentes a tais entradas não serão ativados nem atualizados. Isto pode levar a valores nulos na camada de saída. As maneiras de contornar este problema incluem utilizar variantes da \emph{ReLU} que não exibam saídas nulas, diferentes estratégias de inicialização e regularização de pesos e \emph{batches}, entre outras. Assim, substituiu-se apenas na camada de saída a \emph{ReLU} por uma de suas variantes, chamada \emph{Leaky ReLU}. Esta função de ativação produz saídas negativas frente a estímulos negativos, como é possível observar na Figura \ref{fig:lrelu} \todo{citar artigos}.

Com isto, observa-se uma melhora significativa na performance da AlexNet, enquanto o RMSE da LeNet não sofreu grandes mudanças. Quanto às saídas das redes, a LeNet exibe valores positivos e negativos próximos de zero, e a AlexNet prevê a mesma idade, $36,72$ anos, para qualquer face apresentada. Nota-se que este valor é próximo da média de idade do conjunto de dados, de $38.63$ anos.

Nos próximos meses, os esforços estarão concentrados em pesquisar e adotar estratégias que resolvam os problemas identificados, como substituir as funções de ativação das camadas ocultas por outras variantes da \emph{ReLU}, adotar métodos específicos de inicialização de pesos, normalização de \emph{batch}, entre outros. Planeja-se também a proposição, o treinamento e teste de outras redes inspiradas em modelos canônicos mais robustos já aplicados em tarefas de aprendizado similares, utilizando técnicas de \emph{transfer learning}.
