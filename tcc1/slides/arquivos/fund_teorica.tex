%!TEX root = ../main.tex
% \begin{frame}{\emph{Smart} TVs}
%    \ \  \\[0.1cm]
%   \begin{itemize}
%   \item Capacidades interativas
%   \item Conexão à internet
%   \item Conteúdo de mídia transmitido a partir de outros dispostivo
% \end{itemize}
% \end{frame}
%
% \begin{frame}{\emph{Smart} TVs}
% \begin{figure}[h!]
% 	\includegraphics[width=0.6\textwidth]{img/smart_samsung.jpg}
% 	\caption{Diagrama representativo de uma \emph{Smart} TV e seus componentes}
% 	\label{fig:smart_samsung}
% \end{figure}
% \end{frame}
%
%
% \begin{frame}{\emph{Smart} TVs}
%   \begin{figure}[h!]
%   	\centering
%   	\includegraphics[width=0.6\linewidth]{img/sbt_app.jpg}
%   	\caption{Aplicativo SBT}
%   	\label{fig:sbt_app}
%   \end{figure}
% \end{frame}
%
% \begin{frame}{\emph{Smart} TVs}
%    \ \  \\[0.1cm]
%   \begin{itemize}
%   \item PNAD 2015
%   \begin{itemize}
%     \item $103$ milhões de aparelhos de televisões em residências e pontos comerciais
%     \item $16$ milhões de \emph{Smart} TVs
%     \item $94\%$ destas \emph{Smart} TVs foram adquiridas entre $2014$ e $2015$
%     \item $68,2\%$ do total de televisores vendidos no primeiro semestre de $2017$
%   \end{itemize}
% \end{itemize}
% \end{frame}
%
% \begin{frame}{\emph{Smart} TVs}
%    \ \  \\[0.1cm]
%   \begin{itemize}
%   \item Benefícios resultantes do uso de \emph{Smart}
%
%   \item Encerramento da transmissão de sinal analógico da televisão aberta
%   \item Copa do Mundo 2018
%   \item Tecnologia 4K
% \end{itemize}
% \end{frame}
%
% \begin{frame}{Classificação Indicativa}
%    \ \  \\[0.1cm]
%   \begin{itemize}
%   \item Sistema de garantias dos direitos da criança e do adolescente
%   \item Reserva-se o direito final aos pais e responsáveis
%   \item Órgão responsável: Cocind, vinculada ao Ministério da Justiça
%   \item Análise de classificação indicativa
%   \begin{itemize}
%     \item Grau de incidência de conteúdos impróprios
%   \end{itemize}
% \end{itemize}
% \end{frame}

\begin{frame}{Machine Learning}
   \ \  \\[0.1cm]
  \begin{itemize}
  \item Estudo sistemático de algoritmos e sistemas que são capazes de melhorar seu desempenho com a experiência
  \item Modelo ou função que mapeie as instâncias do espaço de entrada para o de saída
  \item Paradigmas de Aprendizado
  \begin{itemize}
    \item Aprendizado Supervisionado
    \item Aprendizado Não-Supervisionado
    \item Aprendizado por Reforço
  \end{itemize}
  \item Tarefas de Aprendizado
  \begin{itemize}
    \item Classificação
    \item Regressão
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Redes Neurais Artificiais}
   \ \  \\[0.1cm]
  \begin{itemize}
  \item Cérebro humano
  \item Neurônios: unidades de processamento simples
  \item Capacidade de capturar tendências
  \item Generalização
\end{itemize}
\end{frame}

\begin{frame}{Redes Neurais Artificiais}
     \ \  \\[0.1cm]

\begin{figure}
	\caption{Redes neurais biológicas.}
	\begin{minipage}[h]{0.5\linewidth}
          \centering
		\caption{Neurônio biológico e seus componentes.}
		\label{fig:neuronio_biologico}
		\includegraphics[width=0.7\linewidth]{img/neuronio}
	\end{minipage}%
	\begin{minipage}[h]{0.6\linewidth}
          \centering
		\caption{Sinapse entre neurônios.}
		\label{fig:redeneuralbiologica}
		\includegraphics[width=0.7\linewidth]{./img/redeneuralbiologica.jpg}
	\end{minipage}%
\end{figure}
\end{frame}

\begin{frame}{Redes Neurais Artificiais}
   \ \  \\[0.1cm]
  \begin{itemize}
  \item McCulloch e Pitts
  \begin{figure}[ht]
  	\centering
  	\label{fig:neuronio}
  	\includegraphics[width=0.7\textwidth]{img/perceptron.png}
     \caption{Representação de um neurônio artificial}
  \end{figure}
\end{itemize}
\end{frame}

\begin{frame}{Redes Neurais Artificiais}
   \ \  \\[0.1cm]
  \begin{itemize}
  \item Perceptron de Rosenblatt (1958)
  \begin{itemize}
    \item Algoritmo de aprendizado
    \item Endereçar apenas problemas linearmente separáveis
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Redes Neurais Artificiais}
   \ \  \\[0.1cm]
   \begin{figure}[!h]
   	\label{fig:popular_archs}
   	\includegraphics[width=0.6\linewidth]{img/popular_archs}
     \caption{Arquiteturas populares de RNAs}
   \end{figure}
\end{frame}

\begin{frame}{Redes Neurais Artificiais}
   \ \  \\[0.1cm]
   \input{./arquivos/tab_ativacao}
\end{frame}

\begin{frame}{Redes Neurais Artificiais}
   \ \  \\[0.1cm]
   \begin{itemize}
        \item Redes Neurais \emph{Multilayer Feedforward Perceptron}
        \begin{itemize}
             \item Função perda ou Erro
             \item Fase \emph{forward}
             \item Fase \emph{backward}
             \item Atualização dos pesos e \emph{bias}
        \end{itemize}
   \end{itemize}

   \begin{figure}[ht]
   	\centering
   	\label{fig:mlp}
   	\includegraphics[width=0.4\textwidth]{img/mlprna.jpg}
     \caption{Rede Neural MLP com duas camadas ocultas.}
   \end{figure}
\end{frame}

\begin{frame}{Redes Neurais Artificiais}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item Hiperparâmetros de uma RNA
     \begin{itemize}
       \item Taxa de aprendizado
       \item Funções de ativação
       \item Arquitetura da rede
       \item \emph{batch size}
       \item Número de épocas
     \end{itemize}
   \end{itemize}
\end{frame}

% \begin{frame}{Redes Neurais Artificiais}
%    \ \  \\[0.1cm]
%   \begin{itemize}
%   \item Algoritmo de treinamento de uma RNA
%   \item Entrada: Conjuntos de exemplos e respectivos rótulos $(X,Y)$, rede neural a ser treinada, número de épocas $e$, taxa de aprendizado $\eta$ e \emph{batch size} $b$.
%   \item Saida: Rede neural treinada.
%   \begin{enumerate}
%
%     \item Inicialização dos vetores de pesos $w$ e \emph{bias} $b$
%     \item Para cada \emph{batch} = 1,\ldots, b do conjunto de dados:
%     \begin{enumerate}
%       \item Fase \emph{forward}: Calcular previsões $\hat{y}$ e custos $J$.
%       \item Fase \emph{backwards}: Calcular gradientes dos pesos $\nabla_{w^c}$ e \emph{bias} $\nabla_{b^c}$
%       \item Atualizar valores dos pesos e \emph{bias} a partir do gradiente descendente.
%     \end{enumerate}
%   \end{enumerate}
% \end{itemize}
% \end{frame}

\begin{frame}{Deep Learning}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item Representar e reconhecer características sucessivamente complexas
     \item Adição de níveis ou camadas de operações não-lineares
     \item Resolver problemas complexos com um desempenho cada vez maior
     \begin{itemize}
       \item Aumento recente da quantidade de dados disponíveis sobre temas complexos
       \item Aumento da disponibilidade de recursos computacionais para executar modelos mais robustos
     \end{itemize}
   \end{itemize}
\end{frame}

\begin{frame}{Deep Learning}
   \ \  \\[0.1cm]
   \begin{figure}[ht]
   	\centering
   	\label{fig:compara_redes}
   	\includegraphics[width=0.5\textwidth]{img/compara_redes.png}
     \caption{\footnotesize{Evolução de profunidade, taxa de erro e número de parâmetros das redes neurais profundas com o passar dos anos.}}
   \end{figure}
\end{frame}

\begin{frame}{Deep Learning}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item Breve Histórico
     \begin{itemize}
       \item (1950) Modelos lineares simples: McCulloch e Pitts; Perceptron
       \item (1980) Interconexão entre vários neurônios e algoritmo \emph{back-propagation}
       \begin{itemize}
         \item LeNet
       \end{itemize}
       \item (2006) Deep Belief Networks
     \end{itemize}
   \end{itemize}
\end{frame}

\begin{frame}{Deep Learning}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item \emph{ImageNet Large Scale Visual Recognition Challenge} (ILSVRC)
     \begin{itemize}
       \item Imagenet
       \item $14$ milhões de imagens de $21$ mil categorias organizadas hierarquicamente
       \item Erro top-5
     \end{itemize}
   \end{itemize}
\end{frame}

\begin{frame}{Deep Learning}
   \ \  \\[0.1cm]
   \begin{figure}[ht]
   	\centering
   	\caption{Evolução do erro dos modelos vencedores da competição ILSVRC pela profundidade das redes neurais}
   	\label{fig:compara_redes_ilsvrc}
   	\includegraphics[width=0.6\textwidth]{img/compara_redes_ilsvrc.png}
   \end{figure}
\end{frame}

\begin{frame}{Redes Neurais Convolucionais}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item Topologia bem definida e estrutura em grid
     \item Operações de convolução em pelo menos uma de suas camadas
     \item Destaca-se no reconhecimento de padrões em dados de alta dimensionalidade
   \end{itemize}
\end{frame}

\begin{frame}{Redes Neurais Convolucionais}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item Convolução
     \begin{equation}
      S(i,j) = I(i,j)*K(i,j) = \sum_{m}\sum_{n}I(m,n)K(i-m,j-n)\label{eq:conv_img}
     \end{equation}
   \end{itemize}
   \begin{figure}[!h]
   	\centering
   	\label{fig:convolutions}
   	\includegraphics[width=0.8\textwidth]{./img/fundamenta/convolutions}
     \caption{Papel das camadas convolucionais e \emph{feature maps} nas CNNs.}
   \end{figure}
\end{frame}

\begin{frame}{Redes Neurais Convolucionais}
   \ \  \\[0.1cm]
   \begin{figure}
   	\centering
   	\caption{Componentes de uma camada de uma rede neural convolucional.}
   	\label{fig:cnn_camada}
   	\includegraphics[width=\textwidth]{img/cnn_camada_ipe.png}
   \end{figure}
\end{frame}

\begin{frame}{Redes Neurais Convolucionais}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item Hiperparâmetros de uma CNN
     \begin{itemize}
       \item Tamanho e quantidade de filtros
       \item Pooling
       \item Padding
       \item Strides
     \end{itemize}
   \end{itemize}
\end{frame}

\begin{frame}{\LARGE{Modelos Canônicos de Redes Neurais Convolucionais}}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item Arquiteturas que trouxeram contribuições importantes
     \item Comuns ainda hoje no cenário de DL
   \end{itemize}
\end{frame}

\begin{frame}{\LARGE{Modelos Canônicos de Redes Neurais Convolucionais}}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item LeNet (1998)
     \begin{itemize}
       \item Conjunto de dados \emph{Modified National Institute of Standards and Technology} (MNIST)
       \item Imagens em escala de cinza de tamanho $32 \times 32$
       \item Amplamente utilizada por bancos
     \end{itemize}
     \begin{figure}
          \centering
          \includegraphics[width=0.8\textwidth]{img/LENET5}
     \end{figure}
   \end{itemize}
\end{frame}

\begin{frame}{\LARGE{Modelos Canônicos de Redes Neurais Convolucionais}}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item AlexNet (2012)
     \begin{itemize}
       \item Primeira CNN ganhadora do desafio ILSVRC
       \item Imagens de 1000 categorias da ImageNet
       \item Erro top-5 igual a $15.4\%$
       \item Treinamento: duas GPU GTX 580 por 5 a 6 dias
 \end{itemize}
     \begin{figure}
          \centering
          \includegraphics[width=0.8\textwidth]{img/alexnet}
     \end{figure}
   \end{itemize}
\end{frame}

\begin{frame}{\LARGE{Modelos Canônicos de Redes Neurais Convolucionais}}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item VGG (2014)
     \begin{itemize}
       \item Erro top$-$5 de $7.3\%$
       \item Treinamento em 4 GPUs Nvidia Titan Black por duas a três semanas
       \item Erro top$-$5 igual a $15.4\%$
     \end{itemize}
     \begin{figure}
          \centering
          \includegraphics[width=0.8\textwidth]{img/vgg}
     \end{figure}
   \end{itemize}
\end{frame}

\begin{frame}{\LARGE{Modelos Canônicos de Redes Neurais Convolucionais}}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item Inception ou GoogLeNet (2014)
     \begin{itemize}
       \item 22 camadas convolucionais
       \item Treinamento em algumas GPUs de alta performance por uma semana
       \item Erro top-5 de $6.7\%$
     \end{itemize}
     \begin{figure}[h!]
     	\centering
     	\caption{Bloco Inception da CNN GoogLeNet}
     	\includegraphics[width=0.7\linewidth]{img/GoogLeNet}
     	\label{fig:bloco_inception}
     \end{figure}
   \end{itemize}
\end{frame}

\begin{frame}{\LARGE{Modelos Canônicos de Redes Neurais Convolucionais}}
     \ \  \\[0.5cm]
     \begin{figure}[h!]
     	\centering
     	\includegraphics[width=0.7\linewidth]{img/inception}
     	\caption{GoogLeNet}
     \end{figure}
\end{frame}

\begin{frame}{\LARGE{Modelos Canônicos de Redes Neurais Convolucionais}}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item ResNet (2015)
     \begin{itemize}
       \item Total de 152 camadas
       \item Treinamento em 8 GPUs por duas a três semanas
       \item Erro top-5 de $3.6\%$
     \end{itemize}
     \begin{figure}[h!]
     \centering
     \caption{Bloco Residual da CNN ResNet.}\label{fig:bloco_residual}
     \includegraphics[height=0.3\textheight]{img/resnets_modelvariants}
     \end{figure}
   \end{itemize}
\end{frame}

\begin{frame}{Transfer Learning}
   \ \  \\[0.1cm]
     \begin{block}{Transfer Learning}
          Representações de imagens aprendidas por CNNs a partir de conjuntos de dados com grande número de exemplos podem ser transferidas eficientemente para outras tarefas de reconhecimento visual que tenham uma quantidade limitada de dados de treinamento
     \end{block}
\end{frame}

\begin{frame}{Transfer Learning}
   \ \  \\[0.1cm]
   \begin{itemize}
     \item CNNs com alto desempenho
     \item Transferir os parâmetros de peso $w$ e de bias $b$
     \item Remove-se as últimas camadas
   \end{itemize}
\end{frame}
